import os
import json
import pandas as pd
from googleapiclient.discovery import build
from oauth2client.service_account import ServiceAccountCredentials
import gspread
import isodate
from datetime import datetime, timedelta
import pytz
import numpy as np
import requests
# --- ×”×¡×¤×¨×™×™×” ×”×—×“×©×” ---
from google import genai
from google.genai import types

# --- ×”×’×“×¨×•×ª ---
CHANNEL_ID = 'UC_HwfTAcjBESKZRJq6BTCpg'
SHEET_NAME = '× ×ª×•× ×™ ×™×•×˜×™×•×‘'
SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']

def get_youtube_service():
    api_key = os.environ['YOUTUBE_API_KEY']
    return build('youtube', 'v3', developerKey=api_key)

def get_sheet_client():
    creds_json = json.loads(os.environ['GCP_SERVICE_ACCOUNT'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, SCOPES)
    return gspread.authorize(creds)

def format_duration(seconds):
    if seconds == 0: return "0s"
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    if h > 0: return f"{int(h)}h {int(m)}m {int(s)}s"
    elif m > 0: return f"{int(m)}m {int(s)}s"
    else: return f"{int(s)}s"

def get_uploads_playlist_id(youtube):
    try:
        request = youtube.channels().list(part="contentDetails", id=CHANNEL_ID)
        response = request.execute()
        return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']
    except Exception as e:
        print(f"Error finding uploads ID: {e}")
        return None

def get_existing_data():
    try:
        gc = get_sheet_client()
        sh = gc.open_by_url("https://docs.google.com/spreadsheets/d/1WB0cFc2RgR1Z-crjhtkSqLKp1mMdFoby8NwV7h3UN6c/edit")
        try: worksheet = sh.worksheet(SHEET_NAME)
        except: worksheet = sh.get_worksheet(0)
        existing_df = pd.DataFrame(worksheet.get_all_records())
        if not existing_df.empty:
            existing_df['video_id'] = existing_df['video_id'].astype(str)
        return existing_df
    except Exception as e:
        print(f"Error fetching existing data: {e}")
        return pd.DataFrame()

# --- × ×™×ª×•×— AI ×¢× Gemini 3 Pro (×”×’×¨×¡×” ×”×—×“×©×”) ---
def analyze_with_gemini(df, yesterday_date):
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key: return "âš ï¸ ×—×¡×¨ ××¤×ª×— ×œ-Gemini."

    # 1. ××ª×—×•×œ ×”×œ×§×•×— ×”×—×“×© (×”×¨×‘×” ×™×•×ª×¨ × ×§×™!)
    client = genai.Client(api_key=api_key)

    # --- ×”×›× ×ª ×”× ×ª×•× ×™× (×œ×•×’×™×§×” ×–×”×”, ×¨×§ ×”×•×¡×¤×ª × ×ª×•× ×™× ×œ×¤×¨×•××¤×˜) ---
    new_yesterday = df[df['published_at'] == yesterday_date].copy()
    new_yesterday_str = ""
    if not new_yesterday.empty:
        new_yesterday = new_yesterday.sort_values('views', ascending=False)
        for _, row in new_yesterday.head(5).iterrows():
            new_yesterday_str += f"â€¢ {row['title'][:60]} | {row['video_type']} | {row['views']:,} ×¦×¤×™×•×ª\n"
    
    top_delta = ""
    if 'views_delta' in df.columns:
        old_videos = df[df['published_at'] < yesterday_date].copy()
        if not old_videos.empty:
            old_videos['views_delta'] = pd.to_numeric(old_videos['views_delta'], errors='coerce').fillna(0)
            old_videos = old_videos[old_videos['views_delta'] > 0].sort_values('views_delta', ascending=False)
            for _, row in old_videos.head(3).iterrows():
                top_delta += f"â€¢ {row['title'][:60]} | ×-{row['published_at']} | +{int(row['views_delta']):,} ×¦×¤×™×•×ª\n"
    
    total_new = len(new_yesterday)
    total_views_new = new_yesterday['views'].sum() if not new_yesterday.empty else 0
    
    top5_overall = ""
    for _, row in df.nlargest(5, 'views').iterrows():
        marker = "ğŸ†•" if row['published_at'] == yesterday_date else ""
        top5_overall += f"â€¢ {marker}{row['title'][:50]} | {row['video_type']} | {row['views']:,}\n"

    today_date = datetime.now(pytz.timezone('Asia/Jerusalem')).strftime('%d/%m/%Y')
    
        prompt = f"""××ª×” ×›×•×ª×‘ ×“×•×— ×‘×™×¦×•×¢×™ ×™×•×˜×™×•×‘ ×™×•××™ ×œ×¢×¨×•×¥ ×›××Ÿ ×—×“×©×•×ª. ×”×ª××¨×™×š: {today_date}.

=== × ×ª×•× ×™× ===

ğŸ“° ×¡×¨×˜×•× ×™× ×—×“×©×™× (×¢×œ×• ××ª××•×œ {yesterday_date}):
×›××•×ª: {total_new}
×¡×”"×› ×¦×¤×™×•×ª: {total_views_new:,}
×”×¡×¨×˜×•× ×™×:
{new_yesterday_str if new_yesterday_str else "××™×Ÿ ×¡×¨×˜×•× ×™× ×—×“×©×™×"}

ğŸ”¥ ×¡×¨×˜×•× ×™× ×™×©× ×™× ×©×¦×‘×¨×• ×¦×¤×™×•×ª ××ª××•×œ (×“×œ×ª× - ×¦×¤×™×•×ª ×—×“×©×•×ª ×‘×™×××” ×”××—×¨×•× ×”):
{top_delta if top_delta else "××™×Ÿ ××™×“×¢ ×¢×œ ×“×œ×ª×"}

ğŸ“Š ×˜×•×¤ 5 ×›×œ×œ×™ ×‘×¢×¨×•×¥ (×œ×¤×™ ×¡×”×´×› ×¦×¤×™×•×ª ××¦×˜×‘×¨):
{top5_overall}

=== ××‘× ×” ×”×“×•×— ===

×›×ª×•×‘ ×¡×™×›×•× ×©×œ 180-220 ××™×œ×™×. ×”×ª×—×œ ×™×©×¨ ××”×—×œ×§ ×”×§×‘×•×¢, ×‘×œ×™ ×”×§×“××” ××• ×¤×¡×§×ª ×¤×ª×™×—×”.

**×—×œ×§ ×§×‘×•×¢ (×—×•×‘×”):**

ğŸ“Š **×”××¡×¤×¨×™×:** ××©×¤×˜ ××—×“ - ×›××” ×¡×¨×˜×•× ×™× ×¢×œ×• ××ª××•×œ ×•×›××” ×¦×¤×™×•×ª ×¦×‘×¨×•.

ğŸ† **×˜×•×¤ 3 ×××ª××•×œ:**
1. [×©× ××§×•×¦×¨] | [Shorts/×¨×’×™×œ] | [×¦×¤×™×•×ª]
2. [×©× ××§×•×¦×¨] | [Shorts/×¨×’×™×œ] | [×¦×¤×™×•×ª]
3. [×©× ××§×•×¦×¨] | [Shorts/×¨×’×™×œ] | [×¦×¤×™×•×ª]

ğŸ”¥ **×××©×™×š ×œ×”×“×”×“:** ×”×¡×¨×˜×•×Ÿ ×”×™×©×Ÿ (×œ× ×××ª××•×œ) ×©×¦×‘×¨ ×”×›×™ ×”×¨×‘×” ×¦×¤×™×•×ª ××ª××•×œ.
×¤×•×¨××˜: [×©×] | ×¤×•×¨×¡× ×‘-[×ª××¨×™×š] | +[×“×œ×ª×] ×¦×¤×™×•×ª ××ª××•×œ
×× ×”×“×œ×ª× ××¢×œ 5,000 - ×”×•×¡×£ ××©×¤×˜ ×§×¦×¨ ×œ××” ×–×” ×›× ×¨××” ×¢×“×™×™×Ÿ ×¨×œ×•×•× ×˜×™.
×× ××™×Ÿ ××™×“×¢ ×¢×œ ×“×œ×ª× ××• ×©×”×™× × ××•×›×” ×-500 - ×›×ª×•×‘ "××™×Ÿ ×¡×¨×˜×•×Ÿ ×™×©×Ÿ ×‘×•×œ×˜ ×”×™×•×".

**×—×œ×§ ×—×•×¤×©×™ (×—×•×‘×” ×œ×‘×—×•×¨ 2 ×‘× ×•×©××™× ×©×•× ×™×):**

×ª×¡×ª×›×œ ×¢×œ ×”× ×ª×•× ×™× ×•×ª×‘×—×¨ 2 ×ª×•×‘× ×•×ª ××¢× ×™×™× ×•×ª. ×—×©×•×‘: ×”×ª×•×‘× ×•×ª ×—×™×™×‘×•×ª ×œ×”×™×•×ª ×¢×œ × ×•×©××™× ×©×•× ×™× ×œ×’××¨×™.

××¤×©×¨×•×™×•×ª:
- ğŸ“ˆ ××’××” ××• × ×•×©× ×—×
- âš¡ ×”×¤×ª×¢×” - ×¡×¨×˜×•×Ÿ ×©×”×¦×œ×™×—/× ×›×©×œ ××¢×‘×¨ ×œ×¦×¤×•×™
- ğŸ¬ ×ª×¦×¤×™×ª ×¢×œ Shorts vs ×¨×’×™×œ
- ğŸ‘¤ ×§×¨×“×™×˜ ×œ×™×•×¦×¨/×›×ª×‘ ×©××•×–×›×¨ ×‘-description
- ğŸ”„ ×”×©×•×•××” ×œ×™××™× ×§×•×“××™×

×›×ª×•×‘ ×›×œ ×ª×•×‘× ×” ×‘××©×¤×˜-×©× ×™×™×.

=== ×¡×’× ×•×Ÿ ===
- ×”×ª×—×œ ×™×©×¨ ×-ğŸ“Š
- ×§×¦×¨ ×•×¢×•×‘×“×ª×™
- ×¦×™×™×Ÿ ×ª××™×“ ×× ×¡×¨×˜×•×Ÿ ×”×•× Shorts
- ××œ ×ª××¦×™× ×•××œ ×ª× ×—×©
"""


    try:
        # 2. ×”×§×¨×™××” ×”×—×“×©×” ×•×”×§×¦×¨×”
        response = client.models.generate_content(
            model="gemini-3-pro-preview", # ×”××•×“×œ ×”×—×“×© ×©×‘×™×§×©×ª
            contents=prompt,
            config=types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(
                    include_thoughts=True # ×”×¤×¢×œ×ª ×™×›×•×œ×ª ×”×—×©×™×‘×”
                ),
                tools=[types.Tool(google_search=types.GoogleSearch())], # ×”×•×¡×¤×ª ×—×™×¤×•×© ×× ×¦×¨×™×š ×”×©×œ××ª ××™×“×¢
                temperature=0.7
            )
        )
        return response.text
    except Exception as e:
        return f"×©×’×™××” ×‘× ×™×ª×•×— AI: {e}"

def send_telegram_report(df):
    token = os.environ.get('TELEGRAM_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    
    if not token or not chat_id:
        print("Skipping Telegram - missing credentials.")
        return

    il_tz = pytz.timezone('Asia/Jerusalem')
    yesterday = (datetime.now(il_tz) - timedelta(days=1)).strftime('%Y-%m-%d')
    
    print(f"Generating AI report with Gemini 3 Pro for {yesterday}...")
    analysis_text = analyze_with_gemini(df, yesterday)

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {"chat_id": chat_id, "text": analysis_text, "parse_mode": "Markdown"}
    try: 
        response = requests.post(url, json=payload)
        print(f"Telegram response: {response.status_code}")
    except Exception as e: 
        print(f"Telegram Error: {e}")

# --- ×”×¤×•× ×§×¦×™×” ×”×¨××©×™×ª ---
def fetch_videos():
    youtube = get_youtube_service()
    uploads_id = get_uploads_playlist_id(youtube)
    if not uploads_id: return pd.DataFrame()

    il_tz = pytz.timezone('Asia/Jerusalem')
    current_time = datetime.now(il_tz).strftime('%Y-%m-%d %H:%M')
    cutoff_date = datetime.now(pytz.utc) - timedelta(days=30)
    
    videos = []
    next_page = None
    should_stop = False
    
    print("Fetching videos...")
    while True:
        req = youtube.playlistItems().list(part="snippet,contentDetails", playlistId=uploads_id, maxResults=50, pageToken=next_page)
        res = req.execute()
        
        ids_to_fetch = []
        for item in res['items']:
            pub = datetime.strptime(item['contentDetails']['videoPublishedAt'], "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=pytz.utc)
            if pub < cutoff_date: 
                should_stop = True
                break
            ids_to_fetch.append(item['contentDetails']['videoId'])
        
        if not ids_to_fetch: break

        stats_res = youtube.videos().list(part="snippet,contentDetails,statistics,topicDetails", id=','.join(ids_to_fetch)).execute()
        
        for item in stats_res['items']:
            dur = item['contentDetails']['duration']
            try: sec = isodate.parse_duration(dur).total_seconds()
            except: sec = 0
            
            is_short = sec <= 60 and sec > 0
            views = int(item['statistics'].get('viewCount', 0))
            likes = int(item['statistics'].get('likeCount', 0))
            comments = int(item['statistics'].get('commentCount', 0))
            
            thumb = item['snippet']['thumbnails']
            thumb_url = thumb.get('maxres', thumb.get('high', thumb.get('medium')))['url']

            videos.append({
                'video_id': item['id'],
                'published_at': item['snippet']['publishedAt'][:10],
                'published_time': item['snippet']['publishedAt'][11:16],
                'title': item['snippet']['title'],
                'description': item['snippet']['description'],
                'thumbnail_url': thumb_url,
                'tags': ",".join(item['snippet'].get('tags', [])),
                'video_type': 'Shorts' if is_short else '×¨×’×™×œ',
                'views': views,
                'likes': likes,
                'comments': comments,
                'duration_seconds': sec,
                'duration_formatted': format_duration(sec),
                'like_rate': round((likes/views*100) if views > 0 else 0, 2),
                'comment_rate': round((comments/views*100) if views > 0 else 0, 4),
                'video_url': f"https://www.youtube.com/watch?v={item['id']}",
                'last_updated': current_time
            })
            
        if should_stop or 'nextPageToken' not in res: break
        next_page = res['nextPageToken']
            
    print(f"Fetched {len(videos)} videos.")
    return pd.DataFrame(videos)

def update_google_sheet(new_data_df):
    print("Updating Google Sheets...")
    existing_df = get_existing_data()
    
    if not existing_df.empty and 'views' in existing_df.columns:
        existing_df['views'] = pd.to_numeric(existing_df['views'], errors='coerce').fillna(0)
        existing_views = existing_df.set_index('video_id')['views'].to_dict()
        new_data_df['views_delta'] = new_data_df.apply(
            lambda row: row['views'] - existing_views.get(row['video_id'], row['views']), axis=1
        )
    else:
        new_data_df['views_delta'] = 0
    
    gc = get_sheet_client()
    sh = gc.open_by_url("https://docs.google.com/spreadsheets/d/1WB0cFc2RgR1Z-crjhtkSqLKp1mMdFoby8NwV7h3UN6c/edit")
    try: worksheet = sh.worksheet(SHEET_NAME)
    except: worksheet = sh.get_worksheet(0)
    
    if existing_df.empty: final_df = new_data_df
    else:
        new_data_df['video_id'] = new_data_df['video_id'].astype(str)
        existing_df['video_id'] = existing_df['video_id'].astype(str)
        for col in new_data_df.columns:
            if col not in existing_df.columns: existing_df[col] = ""
        combined = pd.concat([new_data_df, existing_df])
        final_df = combined.drop_duplicates(subset=['video_id'], keep='first')
    
    final_df = final_df.sort_values(by='published_at', ascending=False)
    final_df = final_df.fillna(0).replace([np.inf, -np.inf], 0)
    
    for col in ['description', 'tags', 'thumbnail_url', 'published_time', 'duration_formatted']:
        if col in final_df.columns: final_df[col] = final_df[col].replace(0, "")

    worksheet.clear()
    worksheet.update([final_df.columns.values.tolist()] + final_df.values.tolist(), value_input_option='RAW')
    print("Sheet updated successfully!")
    return final_df

if __name__ == "__main__":
    new_videos = fetch_videos()
    if not new_videos.empty:
        updated_df = update_google_sheet(new_videos)
        send_telegram_report(updated_df)
    else:
        print("No videos found.")

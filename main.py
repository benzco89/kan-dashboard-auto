import os
import json
import pandas as pd
from googleapiclient.discovery import build
from oauth2client.service_account import ServiceAccountCredentials
import gspread
import isodate
from datetime import datetime, timedelta
import pytz
import numpy as np
import requests
import google.generativeai as genai

# --- ◊î◊í◊ì◊®◊ï◊™ ---
CHANNEL_ID = 'UC_HwfTAcjBESKZRJq6BTCpg'
SHEET_NAME = '◊†◊™◊ï◊†◊ô ◊ô◊ï◊ò◊ô◊ï◊ë'
SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']

def get_youtube_service():
    api_key = os.environ['YOUTUBE_API_KEY']
    return build('youtube', 'v3', developerKey=api_key)

def get_sheet_client():
    creds_json = json.loads(os.environ['GCP_SERVICE_ACCOUNT'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, SCOPES)
    return gspread.authorize(creds)

def format_duration(seconds):
    if seconds == 0: return "0s"
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    if h > 0: return f"{int(h)}h {int(m)}m {int(s)}s"
    elif m > 0: return f"{int(m)}m {int(s)}s"
    else: return f"{int(s)}s"

def get_uploads_playlist_id(youtube):
    try:
        request = youtube.channels().list(part="contentDetails", id=CHANNEL_ID)
        response = request.execute()
        return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']
    except Exception as e:
        print(f"Error finding uploads ID: {e}")
        return None

# --- ◊†◊ô◊™◊ï◊ó AI ◊ú◊ò◊ú◊í◊®◊ù ---
def analyze_with_gemini(df_recent):
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key: return "‚ö†Ô∏è ◊ó◊°◊® ◊û◊§◊™◊ó ◊ú-Gemini."

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')

    # --- ◊î◊õ◊†◊™ ◊î◊†◊™◊ï◊†◊ô◊ù ◊ú◊†◊ô◊™◊ï◊ó ◊û◊ì◊ï◊ô◊ß ◊ô◊ï◊™◊® ---
    # ◊ô◊¶◊ô◊®◊™ ◊¢◊ï◊™◊ß ◊ú◊†◊ô◊™◊ï◊ó ◊ë◊ú◊ë◊ì
    analysis_df = df_recent[['title', 'description', 'video_type', 'views', 'like_rate', 'comment_rate', 'published_at']].copy()
    
    # ◊î◊û◊®◊î ◊ú◊§◊ï◊®◊û◊ò ◊ß◊®◊ô◊ê ◊ô◊ï◊™◊® ◊ú-AI (◊î◊ï◊°◊§◊™ ◊°◊ô◊û◊†◊ô ◊ê◊ó◊ï◊ñ◊ô◊ù ◊ï◊ß◊ô◊¶◊ï◊® ◊û◊°◊§◊®◊ô◊ù)
    # ◊ñ◊î ◊¢◊ï◊ñ◊® ◊ú◊ï ◊ú◊î◊ë◊ô◊ü ◊§◊®◊ï◊§◊ï◊®◊¶◊ô◊ï◊™ ◊ë◊ú◊ô ◊ú◊†◊ó◊©
    analysis_df['views'] = analysis_df['views'].apply(lambda x: f"{x:,}")
    analysis_df['like_rate'] = analysis_df['like_rate'].astype(str) + '%'
    analysis_df['comment_rate'] = analysis_df['comment_rate'].astype(str) + '%'
    
    data_str = analysis_df.to_string(index=False)
    
    today_date = datetime.now(pytz.timezone('Asia/Jerusalem')).strftime('%d/%m/%Y')

    prompt = f"""
    ◊ê◊™◊î ◊ê◊†◊ú◊ô◊°◊ò ◊ì◊ê◊ò◊î ◊¶◊ô◊†◊ô ◊ï◊û◊ß◊¶◊ï◊¢◊ô ◊©◊ú "◊õ◊ê◊ü ◊ó◊ì◊©◊ï◊™". ◊î◊™◊ê◊®◊ô◊ö: {today_date}.
    ◊î◊û◊ò◊®◊î: ◊ì◊ï◊ó ◊ë◊ï◊ß◊® ◊û◊ë◊ï◊°◊° ◊¢◊ï◊ë◊ì◊ï◊™ ◊ë◊ú◊ë◊ì. ◊ê◊°◊ï◊® ◊ú◊î◊©◊™◊û◊© ◊ë◊ß◊ú◊ô◊©◊ê◊ï◊™ ◊©◊ô◊ï◊ï◊ß◊ô◊ï◊™ ("◊î◊ß◊î◊ú ◊¶◊û◊ê", "◊™◊ï◊õ◊ü ◊û◊®◊í◊©").

    ◊î◊†◊î ◊î◊†◊™◊ï◊†◊ô◊ù ◊î◊í◊ï◊ú◊û◊ô◊ô◊ù (◊©◊ô◊ù ◊ú◊ë ◊ú◊¢◊û◊ï◊ì◊ï◊™ like_rate ◊ï-comment_rate - ◊î◊ü ◊î◊û◊ì◊ì ◊ú◊ê◊ô◊õ◊ï◊™):
    {data_str}

    ◊†◊™◊ó ◊ï◊õ◊™◊ï◊ë ◊ì◊ï◊ó ◊ß◊¶◊® (◊¢◊ì 180 ◊û◊ô◊ú◊ô◊ù) ◊ú◊§◊ô ◊î◊°◊¢◊ô◊§◊ô◊ù ◊î◊ë◊ê◊ô◊ù:

    1. üìä **◊°◊ò◊ò◊ï◊° ◊ô◊ï◊û◊ô:** ◊î◊ê◊ù ◊î◊û◊°◊§◊®◊ô◊ù ◊í◊ë◊ï◊î◊ô◊ù ◊ê◊ï ◊†◊û◊ï◊õ◊ô◊ù ◊û◊î◊®◊í◊ô◊ú ◊ë◊®◊©◊ô◊û◊î ◊î◊ñ◊ï?
    2. üíé **◊ê◊ô◊õ◊ï◊™ ◊û◊ï◊ú ◊õ◊û◊ï◊™ (◊ó◊©◊ï◊ë!):** ◊ñ◊î◊î ◊°◊®◊ò◊ï◊ü ◊©◊î◊ô◊ï ◊ú◊ï ◊¶◊§◊ô◊ï◊™ ◊í◊ë◊ï◊î◊ï◊™ ◊ê◊ë◊ú ◊ê◊ó◊ï◊ñ ◊ú◊ô◊ô◊ß◊ô◊ù/◊™◊í◊ï◊ë◊ï◊™ ◊†◊û◊ï◊ö (◊ß◊ú◊ô◊ß◊ë◊ô◊ô◊ò/◊û◊ó◊ú◊ï◊ß◊™), ◊ê◊ï ◊ú◊î◊§◊ö - ◊°◊®◊ò◊ï◊ü ◊¢◊ù ◊û◊¢◊ò ◊¶◊§◊ô◊ï◊™ ◊ê◊ë◊ú ◊ê◊ó◊ï◊ñ ◊û◊¢◊ï◊®◊ë◊ï◊™ ◊ó◊®◊ô◊í ◊ï◊í◊ë◊ï◊î (◊ß◊î◊ú ◊†◊ê◊û◊ü). ◊™◊ü ◊ì◊ï◊í◊û◊î ◊°◊§◊¶◊ô◊§◊ô◊™.
    3. üéôÔ∏è **◊ë◊ô◊¶◊ï◊¢◊ô ◊õ◊™◊ë◊ô◊ù:** ◊ó◊§◊© ◊©◊û◊ï◊™ ◊ë◊™◊ô◊ê◊ï◊®. ◊û◊ô ◊î◊õ◊™◊ë ◊©◊î◊ë◊ô◊ê ◊ê◊™ ◊î◊û◊°◊§◊®◊ô◊ù ◊î◊õ◊ô ◊ó◊ñ◊ß◊ô◊ù ◊ê◊™◊û◊ï◊ú? (◊¶◊ô◊ô◊ü ◊ê◊™ ◊î◊©◊ù ◊ï◊î◊û◊°◊§◊®).
    4. üß© **◊†◊ô◊™◊ï◊ó ◊†◊ï◊©◊ê◊ô:** ◊ß◊ë◊• ◊ê◊™ ◊î◊°◊®◊ò◊ï◊†◊ô◊ù ◊ú◊§◊ô ◊†◊ï◊©◊ê◊ô◊ù (◊ú◊û◊©◊ú: ◊¶◊ë◊ê, ◊õ◊ú◊õ◊ú◊î, ◊§◊ï◊ú◊ô◊ò◊ô◊ß◊î). ◊ê◊ô◊ñ◊î "◊ß◊ú◊°◊ò◊®" (◊ß◊ë◊ï◊¶◊™ ◊†◊ï◊©◊ê◊ô◊ù) ◊†◊ô◊¶◊ó ◊ë◊û◊û◊ï◊¶◊¢ ◊¶◊§◊ô◊ï◊™? ◊ê◊ú ◊™◊†◊ó◊© ◊ú◊û◊î, ◊®◊ß ◊¶◊ô◊ô◊ü ◊ê◊™ ◊î◊¢◊ï◊ë◊ì◊î.

    ◊°◊í◊†◊ï◊ü:
    * ◊ß◊¶◊®, ◊ó◊ì, ◊û◊ë◊ï◊°◊° ◊û◊°◊§◊®◊ô◊ù.
    * ◊ë◊ú◊ô "◊û◊ô◊ú◊ô◊ù ◊ô◊§◊ï◊™". ◊ê◊ù ◊û◊©◊î◊ï ◊†◊õ◊©◊ú - ◊™◊í◊ô◊ì ◊©◊†◊õ◊©◊ú.
    """

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"◊©◊í◊ô◊ê◊î ◊ë◊†◊ô◊™◊ï◊ó: {e}"    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"◊©◊í◊ô◊ê◊î ◊ë◊†◊ô◊™◊ï◊ó: {e}"

def send_telegram_report(df):
    token = os.environ.get('TELEGRAM_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    
    if not token or not chat_id:
        print("Skipping Telegram.")
        return

    il_tz = pytz.timezone('Asia/Jerusalem')
    # ◊ú◊ï◊ß◊ó◊ô◊ù ◊î◊ß◊©◊® ◊©◊ú 3 ◊ô◊û◊ô◊ù ◊ê◊ó◊®◊ï◊†◊ô◊ù ◊ú◊†◊ô◊™◊ï◊ó
    cutoff = (datetime.now(il_tz) - timedelta(days=3)).strftime('%Y-%m-%d')
    df['date_obj'] = pd.to_datetime(df['published_at'])
    recent_context = df[df['published_at'] >= cutoff].head(25)

    print("Analyzing with Gemini...")
    analysis_text = analyze_with_gemini(recent_context)

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {"chat_id": chat_id, "text": analysis_text, "parse_mode": "Markdown"}
    try: requests.post(url, json=payload)
    except Exception as e: print(f"Telegram Error: {e}")

# --- ◊î◊§◊ï◊†◊ß◊¶◊ô◊î ◊î◊®◊ê◊©◊ô◊™ ---
def fetch_videos():
    youtube = get_youtube_service()
    uploads_id = get_uploads_playlist_id(youtube)
    if not uploads_id: return pd.DataFrame()

    il_tz = pytz.timezone('Asia/Jerusalem')
    current_time = datetime.now(il_tz).strftime('%Y-%m-%d %H:%M')
    cutoff_date = datetime.now(pytz.utc) - timedelta(days=30)
    
    videos = []
    next_page = None
    should_stop = False
    
    print("Fetching videos...")
    while True:
        req = youtube.playlistItems().list(part="snippet,contentDetails", playlistId=uploads_id, maxResults=50, pageToken=next_page)
        res = req.execute()
        
        ids_to_fetch = []
        for item in res['items']:
            pub = datetime.strptime(item['contentDetails']['videoPublishedAt'], "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=pytz.utc)
            if pub < cutoff_date: 
                should_stop = True
                break
            ids_to_fetch.append(item['contentDetails']['videoId'])
        
        if not ids_to_fetch: break

        stats_res = youtube.videos().list(part="snippet,contentDetails,statistics,topicDetails", id=','.join(ids_to_fetch)).execute()
        
        for item in stats_res['items']:
            dur = item['contentDetails']['duration']
            try: sec = isodate.parse_duration(dur).total_seconds()
            except: sec = 0
            
            is_short = sec <= 60 and sec > 0
            
            views = int(item['statistics'].get('viewCount', 0))
            likes = int(item['statistics'].get('likeCount', 0))
            comments = int(item['statistics'].get('commentCount', 0))
            
            thumb = item['snippet']['thumbnails']
            thumb_url = thumb.get('maxres', thumb.get('high', thumb.get('medium')))['url']

            videos.append({
                'video_id': item['id'],
                'published_at': item['snippet']['publishedAt'][:10],
                'published_time': item['snippet']['publishedAt'][11:16],
                'title': item['snippet']['title'],
                'description': item['snippet']['description'],
                'thumbnail_url': thumb_url,
                'tags': ",".join(item['snippet'].get('tags', [])),
                'video_type': 'Shorts' if is_short else '◊®◊í◊ô◊ú',
                'views': views,
                'likes': likes,
                'comments': comments,
                'duration_seconds': sec,
                'duration_formatted': format_duration(sec),
                'like_rate': round((likes/views*100) if views>0 else 0, 2),
                'comment_rate': round((comments/views*100) if views>0 else 0, 4),
                'video_url': f"https://www.youtube.com/watch?v={item['id']}",
                'last_updated': current_time
            })
            
        if should_stop or 'nextPageToken' not in res: break
        next_page = res['nextPageToken']
            
    return pd.DataFrame(videos)

def update_google_sheet(new_data_df):
    print("Updating Google Sheets...")
    gc = get_sheet_client()
    sh = gc.open_by_url("https://docs.google.com/spreadsheets/d/1WB0cFc2RgR1Z-crjhtkSqLKp1mMdFoby8NwV7h3UN6c/edit")
    try: worksheet = sh.worksheet(SHEET_NAME)
    except: worksheet = sh.get_worksheet(0)
    
    existing_df = pd.DataFrame(worksheet.get_all_records())
    
    if existing_df.empty: final_df = new_data_df
    else:
        new_data_df['video_id'] = new_data_df['video_id'].astype(str)
        existing_df['video_id'] = existing_df['video_id'].astype(str)
        for col in new_data_df.columns:
            if col not in existing_df.columns: existing_df[col] = ""
        combined = pd.concat([new_data_df, existing_df])
        final_df = combined.drop_duplicates(subset=['video_id'], keep='first')
    
    final_df = final_df.sort_values(by='published_at', ascending=False)
    final_df = final_df.fillna(0).replace([np.inf, -np.inf], 0)
    
    for col in ['description', 'tags', 'thumbnail_url', 'published_time', 'duration_formatted']:
        if col in final_df.columns: final_df[col] = final_df[col].replace(0, "")

    worksheet.clear()
    worksheet.update([final_df.columns.values.tolist()] + final_df.values.tolist(), value_input_option='RAW')
    print("Success!")

if __name__ == "__main__":
    new_videos = fetch_videos()
    if not new_videos.empty:
        update_google_sheet(new_videos)
        send_telegram_report(new_videos)
    else:
        print("No videos found.")
